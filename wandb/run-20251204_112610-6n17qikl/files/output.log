[34m[1mwandb[0m: Detected [huggingface_hub.inference] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
num_proc must be <= 27. Reducing num_proc to 27 for dataset of size 27.
[datasets.arrow_dataset|WARNING]num_proc must be <= 27. Reducing num_proc to 27 for dataset of size 27.
Starting Unsloth training...
The model is already on multiple devices. Skipping the move to device specified in `args`.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 483 | Num Epochs = 2 | Total steps = 122
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)
  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                              | 10/122 [00:16<02:22,  1.27s/it]Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.
{'loss': 2.7541, 'grad_norm': 2.087409257888794, 'learning_rate': 0.00013846153846153847, 'epoch': 0.17}
Using gradient accumulation will be very slightly less accurate.
Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                      | 61/122 [01:22<01:24,  1.39s/it]`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122/122 [02:42<00:00,  1.20s/it]`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192
{'eval_loss': 1.5094740390777588, 'eval_runtime': 1.2354, 'eval_samples_per_second': 21.854, 'eval_steps_per_second': 5.666, 'epoch': 0.17}
{'loss': 1.0812, 'grad_norm': 0.8853130340576172, 'learning_rate': 0.0001889908256880734, 'epoch': 0.33}
{'eval_loss': 0.7872527837753296, 'eval_runtime': 0.7473, 'eval_samples_per_second': 36.13, 'eval_steps_per_second': 9.367, 'epoch': 0.33}
{'loss': 0.7917, 'grad_norm': 0.8065876364707947, 'learning_rate': 0.0001706422018348624, 'epoch': 0.5}
{'eval_loss': 0.7304370999336243, 'eval_runtime': 0.7466, 'eval_samples_per_second': 36.164, 'eval_steps_per_second': 9.376, 'epoch': 0.5}
{'loss': 0.7473, 'grad_norm': 0.76866614818573, 'learning_rate': 0.00015229357798165138, 'epoch': 0.66}
{'eval_loss': 0.6972293257713318, 'eval_runtime': 0.7482, 'eval_samples_per_second': 36.084, 'eval_steps_per_second': 9.355, 'epoch': 0.66}
{'loss': 0.7045, 'grad_norm': 0.6329913139343262, 'learning_rate': 0.00013394495412844036, 'epoch': 0.83}
{'eval_loss': 0.6799701452255249, 'eval_runtime': 0.7468, 'eval_samples_per_second': 36.154, 'eval_steps_per_second': 9.373, 'epoch': 0.83}
{'loss': 0.6943, 'grad_norm': 0.535162627696991, 'learning_rate': 0.00011559633027522936, 'epoch': 0.99}
{'eval_loss': 0.6511321067810059, 'eval_runtime': 0.752, 'eval_samples_per_second': 35.904, 'eval_steps_per_second': 9.308, 'epoch': 0.99}
{'loss': 0.568, 'grad_norm': 0.7632933855056763, 'learning_rate': 9.724770642201836e-05, 'epoch': 1.15}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122/122 [02:43<00:00,  1.34s/it]
{'eval_loss': 0.6197850108146667, 'eval_runtime': 0.7518, 'eval_samples_per_second': 35.915, 'eval_steps_per_second': 9.311, 'epoch': 1.15}
{'loss': 0.5358, 'grad_norm': 0.6865327954292297, 'learning_rate': 7.889908256880735e-05, 'epoch': 1.31}
{'eval_loss': 0.5677419304847717, 'eval_runtime': 0.7564, 'eval_samples_per_second': 35.694, 'eval_steps_per_second': 9.254, 'epoch': 1.31}
{'loss': 0.5065, 'grad_norm': 0.6950795650482178, 'learning_rate': 6.055045871559634e-05, 'epoch': 1.48}
{'eval_loss': 0.5642575621604919, 'eval_runtime': 0.7491, 'eval_samples_per_second': 36.044, 'eval_steps_per_second': 9.345, 'epoch': 1.48}
{'loss': 0.4594, 'grad_norm': 0.5374504327774048, 'learning_rate': 4.2201834862385324e-05, 'epoch': 1.64}
{'eval_loss': 0.5622175931930542, 'eval_runtime': 0.748, 'eval_samples_per_second': 36.097, 'eval_steps_per_second': 9.358, 'epoch': 1.64}
{'loss': 0.46, 'grad_norm': 0.517713725566864, 'learning_rate': 2.3853211009174313e-05, 'epoch': 1.81}
{'eval_loss': 0.5608556270599365, 'eval_runtime': 0.7484, 'eval_samples_per_second': 36.075, 'eval_steps_per_second': 9.353, 'epoch': 1.81}
{'loss': 0.4706, 'grad_norm': 0.4963500499725342, 'learning_rate': 5.504587155963303e-06, 'epoch': 1.98}
{'eval_loss': 0.5601250529289246, 'eval_runtime': 0.7476, 'eval_samples_per_second': 36.117, 'eval_steps_per_second': 9.364, 'epoch': 1.98}
{'train_runtime': 163.6877, 'train_samples_per_second': 5.901, 'train_steps_per_second': 0.745, 'train_loss': 0.8086635993152368, 'epoch': 2.0}
`rope_scaling`'s original_max_position_embeddings field must be less than max_position_embeddings, got 8192 and max_position_embeddings=8192
Training finished and model saved.
